{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6280e1d",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fb69e",
   "metadata": {},
   "source": [
    " LangChain Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "doc =Document(\n",
    "    page_content=\"this is the official page content.\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Krish Naik\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d02fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os \n",
    "os.makedirs(\"../updatedgenerativeai/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = {\n",
    "    \"../updatedgenerativeai/text_files/python.txt\":\"\"\"python is a high level interpreted programming language\n",
    "    key Feature of python:\n",
    "    1. Easy to learn and use\n",
    "    2. Large standard library\n",
    "    3. Platform independent\n",
    "    4. Open source\n",
    "    5. Large community\n",
    "    6. Dynamic typing\n",
    "    7. Automatic memory management\n",
    "    8. Object oriented\n",
    "    9. Interpreted\n",
    "    10. High level\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath,content  in sample_text.items():\n",
    "    with open(filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Created {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../updatedgenerativeai/text_files/python.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"../updatedgenerativeai/text_files\", glob=\"*.txt\",loader_cls=TextLoader,show_progress=True)\n",
    "dir_docs = loader.load()\n",
    "dir_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "loader = DirectoryLoader(\"../updatedgenerativeai/pdf\",\n",
    "glob=\"**/*.pdf\",\n",
    "loader_cls=PyMuPDFLoader,\n",
    "show_progress=True)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379b6f5",
   "metadata": {},
   "source": [
    "RAG System Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read All The pdf inside the directory\n",
    "\n",
    "def process_all_pdf(pdf_directory):\n",
    "    \"\"\"Process all The PDF files in the given directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing {pdf_file.name}\")\n",
    "        try :\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source\"] = pdf_file.name\n",
    "                doc.metadata[\"file_type\"] = \"pdf\"\n",
    "                \n",
    "            all_documents.extend(docs)\n",
    "            print(f\"Processed {pdf_file.name} ({len(docs)} pages)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents processed: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55039c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = process_all_pdf(\"../updatedgenerativeai/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Splitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,length_function=len, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca66ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd547d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets do The Embedding\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b398535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name :str =\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the embedding manager\n",
    "        Args:\n",
    "            model_name (str, optional): The name of the model to use. Defaults to \"all-MiniLM-L6-v2\".\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load  the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            self.model =  SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model: {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_embedding(self,text:List[str]):\n",
    "        \"\"\" \n",
    "        Generate the embeddings for the given text\n",
    "        Args : \n",
    "        texts:List of text strings to embed\n",
    "        returns : \n",
    "        numpy array of embeddings with shape (num_texts,embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Please load the model first.\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings=self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c593bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Embedding Manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b05835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da8007dc",
   "metadata": {},
   "source": [
    "## Vector Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdfc84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc77128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from typing import List, Any\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings and retrieval using ChromaDB\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"../updatedgenerativeai/vector_store\",\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "\n",
    "            #  Correct client initialization\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=self.persist_directory\n",
    "            )\n",
    "\n",
    "            #  Correct collection creation\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF documents embeddings for RAG\"},\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized: {self.collection_name}\")\n",
    "            print(f\"Existing documents: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and embeddings to the vector store\"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Documents and embeddings count mismatch\")\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                documents=documents_text,\n",
    "                metadatas=metadatas,\n",
    "            )\n",
    "            print(f\"Successfully added {len(ids)} documents\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def query(self, query_embedding: np.ndarray, k: int = 5):\n",
    "        \"\"\"Query similar documents\"\"\"\n",
    "        try:\n",
    "            return self.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=k,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying vector store: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad865ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converts chunks to embeddings\n",
    "\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate Embeddings\n",
    "\n",
    "embeddings=embedding_manager.get_embedding(texts)\n",
    "\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241df39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##store in vectore dbstore\n",
    "\n",
    "vector.add_documents(chunks,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec0b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1e042f7",
   "metadata": {},
   "source": [
    "# Retriever Pipline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273da812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval of relevant documents from a vector store.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        score_threshold: float = 0.0,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents based on a query.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"Top k: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_manager.get_embedding([query])[0]\n",
    "\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\", \"ids\"],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying vector store: {e}\")\n",
    "            return []\n",
    "\n",
    "        retrieved_docs = []\n",
    "\n",
    "        if not results[\"documents\"] or not results[\"documents\"][0]:\n",
    "            print(\"No documents retrieved\")\n",
    "            return []\n",
    "\n",
    "        documents = results[\"documents\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "        distances = results[\"distances\"][0]\n",
    "        ids = results[\"ids\"][0]\n",
    "\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(\n",
    "            zip(ids, documents, metadatas, distances)\n",
    "        ):\n",
    "            # Assumes cosine distance\n",
    "            similarity = 1.0 - distance\n",
    "\n",
    "            if similarity >= score_threshold:\n",
    "                retrieved_docs.append(\n",
    "                    {\n",
    "                        \"id\": doc_id,\n",
    "                        \"document\": doc,\n",
    "                        \"metadata\": metadata,\n",
    "                        \"similarity_score\": similarity,\n",
    "                        \"distance\": distance,\n",
    "                        \"rank\": i + 1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "        return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dfae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(\"\\n--- RETRIEVE DEBUG ---\")\n",
    "        print(\"Query:\", query)\n",
    "        print(\"Collection count:\", self.vector_store.collection.count())\n",
    "\n",
    "        query_embedding = self.embedding_manager.get_embedding([query])[0]\n",
    "        print(\"Query embedding dim:\", len(query_embedding))\n",
    "\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "        )\n",
    "\n",
    "        # Print raw shapes\n",
    "        print(\"Raw keys:\", results.keys())\n",
    "        print(\"IDs returned:\", len(results[\"ids\"][0]) if results.get(\"ids\") else None)\n",
    "        print(\"Docs returned:\", len(results[\"documents\"][0]) if results.get(\"documents\") else None)\n",
    "        print(\"Distances:\", results[\"distances\"][0] if results.get(\"distances\") else None)\n",
    "\n",
    "        retrieved_docs = []\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(\n",
    "            zip(results[\"ids\"][0], results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0])\n",
    "        ):\n",
    "            # Keep your conversion, but print it\n",
    "            similarity = 1.0 - distance\n",
    "            print(f\"Rank {i+1}: distance={distance:.4f}, similarity={similarity:.4f}\")\n",
    "\n",
    "            if similarity >= score_threshold:\n",
    "                retrieved_docs.append({\n",
    "                    \"id\": doc_id,\n",
    "                    \"document\": doc,\n",
    "                    \"metadata\": metadata,\n",
    "                    \"similarity_score\": similarity,\n",
    "                    \"distance\": distance,\n",
    "                    \"rank\": i + 1,\n",
    "                })\n",
    "\n",
    "        print(\"Returned after threshold:\", len(retrieved_docs))\n",
    "        print(\"--- END DEBUG ---\\n\")\n",
    "        return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore(\n",
    "    collection_name=\"pdf_documents\",\n",
    "    persist_directory=\"../updatedgenerativeai/vector_store\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_manager = EmbeddingManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(\n",
    "    vector_store=vector_store,\n",
    "    embedding_manager=embedding_manager\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675be7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(\"what  is his strong skill Asfand?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3e8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "537098e8",
   "metadata": {},
   "source": [
    "# integration of Vectrdb context pipeline with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a60ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3838ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### simple RAG system pipeline\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", groq_api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "retriever = RAGRetriever(vector_store, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baef97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple RAG function :retireve context + generate response\n",
    "\n",
    "def reg_simple(query,retriever,llm,top_k=3):\n",
    "    # retriever the context \n",
    "    results = retriever.retrieve(query,top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['document'] for doc in results]) if results else \"No context found\"\n",
    "    # generate response\n",
    "    prompt = f\"\"\"Answer the following question based on the context provided:\n",
    "    Context: {context}\n",
    "    Question: {query}\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affeb65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anse=reg_simple(\"who is asfand? his strongest quality\",retriever,llm)\n",
    "print(anse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff20ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc1088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
