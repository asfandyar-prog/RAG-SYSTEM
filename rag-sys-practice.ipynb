{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b2728f",
   "metadata": {},
   "source": [
    "PipeLine of Rag System\n",
    "\n",
    "1. loading the documents\n",
    "2. Text Preprocessing\n",
    "3. Text chunking\n",
    "4. Embedding Generation\n",
    "5.vector store \n",
    "6. Query Processing\n",
    "7. Similarity Search\n",
    "8. Answer Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0941a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa6fa6",
   "metadata": {},
   "source": [
    "# loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader =DirectoryLoader(\"../updatedgenerativeai/pdf\",glob=\"**/*.pdf\",loader_cls=PyMuPDFLoader,show_progress=True)\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader,PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read All The pdf inside the directory\n",
    "def process_all_pdf(pdf_directory):\n",
    "    \"\"\" Process All The PDF files in the given Directory\"\"\" \n",
    "    all_documents = []\n",
    "    pdf_dir=Path(pdf_directory)\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            loader=PyMuPDFLoader(pdf_file)\n",
    "            docs=loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source\"]=pdf_file.name\n",
    "                doc.metadata[\"file_type\"]=\"pdf\"\n",
    "            all_documents.append(doc)\n",
    "            print(f\"Processed {pdf_file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {pdf_file}: {e}\")\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents=process_all_pdf(\"../updatedgenerativeai/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making Chunks of the text\n",
    "\n",
    "def split_text_in_chunks(documents,chunk_size=2000,chunk_overlap=20):\n",
    "    \"\"\" Break each document in chunks so to process it properly\"\"\"\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,length_function=len,separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    doc_splitter=text_splitter.split_documents(documents)\n",
    "    print(f\"splitted the {len(documents)} documents into {len(doc_splitter)} chunks\")\n",
    "    if doc_splitter:\n",
    "        print(\"\\n Example split of first chunk\")\n",
    "        print(doc_splitter[0].page_content)\n",
    "        print(\"\\n Example split of second chunk\")\n",
    "        print(doc_splitter[1].page_content)\n",
    "    return doc_splitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=split_text_in_chunks(all_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538e220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe7602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets do the embeddings\n",
    "\n",
    "import numpy as np \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List,Dict,Any,Tuple\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e795997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager :\n",
    "    def __init__(self,model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading the model {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_embeddings(self,text:List[str]):\n",
    "        \"\"\"Generate the embeddings for the given text\n",
    "            args: \n",
    "                text (list):list of strings\n",
    "            returns:\n",
    "                np.ndarray: embeddings of the given text\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        try:\n",
    "            embeddings = self.model.encode(text,show_progress_bar=True)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error while generating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0dd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=EmbeddingManager()\n",
    "\n",
    "embed.get_embeddings([\"hello\",\"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b0eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9671d30b",
   "metadata": {},
   "source": [
    "# Vector Store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a134891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Managing the embeddings we created\"\"\" \n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name:str =\"pdf_documents\",\n",
    "        persist_directory:str =\"./vector_store\"\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the vector store\"\"\" \n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(\n",
    "                path=self.persist_directory\n",
    "            )\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF documents embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized: {self.collection_name}\")\n",
    "            print(f\"Existing documents: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def add_documents(self,documents:list[Any],embeddings:np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\"\"\" \n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Documents and embeddings count mismatch\")\n",
    "        ids=[]\n",
    "        embeddings_list=[]\n",
    "        documents_text=[]\n",
    "        metadata=[]\n",
    "\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                documents=documents_text,\n",
    "                metadatas=metadatas,\n",
    "            )\n",
    "            print(f\"Successfully added {len(ids)} documents\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def query(self, query_embedding: np.ndarray, k: int = 5):\n",
    "        \"\"\"Query similar documents\"\"\"\n",
    "        try:\n",
    "            return self.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=k,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying vector store: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53722129",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f47b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=VectorStore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings=embed.get_embeddings(texts)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c6745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa414820",
   "metadata": {},
   "source": [
    "Retreival Pipeline from vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7641c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
