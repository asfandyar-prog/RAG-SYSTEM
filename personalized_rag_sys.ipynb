{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,PyMuPDFLoader\n",
    "\n",
    "loader = DirectoryLoader(\"../Single-Source/papers\",glob=\"**/*.pdf\",loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ecc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading all  the documents from the directory\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_all_docs(directory=\"../Single-Source/papers\"):\n",
    "    \"\"\"Read all the documents from the papers directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir=Path(directory)\n",
    "    pdf_files=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try: \n",
    "            loader= PyMuPDFLoader(str(pdf_file))\n",
    "            docs=loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source\"]=pdf_file\n",
    "                doc.metadata[\"file_name\"]=pdf_file.name\n",
    "                all_documents.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_file}: {e}\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents=read_all_docs()\n",
    "print(f\"\\nRead {len(all_documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making chunkings of the documents\n",
    "\n",
    "def make_chunks(document,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Make chunks of the documents\"\"\" \n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(document)\n",
    "    if chunks:\n",
    "        print(f\"Document split into {len(chunks)} chunks\")\n",
    "        print(f\"First chunk: {chunks[0].page_content}\")\n",
    "        print(f\"Last chunk: {chunks[-1].page_content}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de399b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkss=make_chunks(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets makes the Embeddings\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from  typing import List,Dict,Any,Tuple\n",
    "import chromadb\n",
    "import uuid\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e071cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Make the embeddings for the documents\"\"\" \n",
    "    def __init__(self,model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\" Load the model\"\"\" \n",
    "        try:\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"Model {self.model_name} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "        \n",
    "\n",
    "    def get_embeddings(self,text:List[str]):\n",
    "        \"\"\" \n",
    "        Generate the embeddings for the given text\n",
    "        Args : \n",
    "        texts:List of text strings to embed\n",
    "        returns : \n",
    "        numpy array of embeddings with shape (num_texts,embedding_dim)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Please load the model first\")\n",
    "        embeddings=self.model.encode(text,show_progress_bar=True)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49321bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create  a vector base\n",
    "import chromadb\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "from typing import List, Any\n",
    "\n",
    "class VectorBase:\n",
    "    \"\"\"VectorBase class to store and retrieve embeddings\"\"\" \n",
    "    def __init__(self,collection_name:str=\"pdf_documents\",persist_directory:str=\"./vector_store\"):\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_client()\n",
    "    def _initialize_client(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(\n",
    "                path=self.persist_directory\n",
    "            )\n",
    "            self.colletion=chromadb.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF documents embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized: {self.collection_name}\")\n",
    "            print(f\"Existing documents: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        \"\"\"Add documents to the vector store\"\"\" \n",
    "        if len(documents)!=len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        ids=[]\n",
    "        documents_text=[]\n",
    "        embeddings_list=[]\n",
    "        metadatas=[]\n",
    "        for i,doc in enumerate(zip(documents,embeddings)):                                                                                                          \n",
    "\n",
    "            doc_id=f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            metadata=dict(doc.metadata)\n",
    "            metadata[\"doc_index\"]=i\n",
    "            metadata[\"content_length\"]=len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embeddings.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "\n",
    "            print(f\"Added {len(documents)} documents to vector store\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "\n",
    "\n",
    "    def query(self,query:np.ndarray,k:int=5):\n",
    "        \"\"\" Query the vector store for similar documents\"\"\"\n",
    "        try:\n",
    "            results=self.collection.query(\n",
    "                query_embeddings=query,\n",
    "                n_results=k\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying vector store: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f43ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and Generate\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self,VectorBase,EmbeddingManager):\n",
    "        self.vector_base=VectorBase\n",
    "        self.embedding_manager=EmbeddingManager\n",
    "\n",
    "    def retrieve(self,query:str,top_k:int=5,score_threshold:float=0.0)->List[Dict[Any]]:\n",
    "        query_embedding=self.embedding_manager.get_embedding([query])[0]\n",
    "        results=self.vector_base.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k,\n",
    "            include=[\"metadatas\",\"documents\",\"distances\"]\n",
    "        )\n",
    "        retrieved_docs=[]\n",
    "        for i,(doc_id,doc,metadata,distance) in enumerate(zip(results[\"doc_id\"][0],results[\"metadatas\"][0],results[\"documents\"][0])):\n",
    "             # Keep your conversion, but print it\n",
    "            similarity = 1.0 - distance\n",
    "            print(f\"Rank {i+1}: distance={distance:.4f}, similarity={similarity:.4f}\")\n",
    "\n",
    "            if similarity >= score_threshold:\n",
    "                retrieved_docs.append({\n",
    "                    \"id\": doc_id,\n",
    "                    \"document\": doc,\n",
    "                    \"metadata\": metadata,\n",
    "                    \"similarity_score\": similarity,\n",
    "                    \"distance\": distance,\n",
    "                    \"rank\": i + 1,\n",
    "                })\n",
    "\n",
    "        return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e58fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### simple RAG system pipeline\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", groq_api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "retriever = RAGRetriever(vector_store, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6867bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple RAG function :retireve context + generate response\n",
    "\n",
    "def reg_simple(query,retriever,llm,top_k=3):\n",
    "    # retriever the context \n",
    "    results = retriever.retrieve(query,top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['document'] for doc in results]) if results else \"No context found\"\n",
    "    # generate response\n",
    "    prompt = f\"\"\"Answer the following question based on the context provided:\n",
    "    Context: {context}\n",
    "    Question: {query}\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c41ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
